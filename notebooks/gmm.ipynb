{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is solely dedicated to create a GMM from scratch that is friendly to newbies. \n",
    "So, the mathematical definitions are included as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are trying to compute theta\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\theta = \\{ \\mu_m,\\ \\Sigma_m,\\ \\pi_m \\} \\ \\text{for each class } m$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, Latex\n",
    "# Display the GMM parameter definition\n",
    "print(\"We are trying to compute theta\")\n",
    "display(Latex(r'$\\theta = \\{ \\mu_m,\\ \\Sigma_m,\\ \\pi_m \\} \\ \\text{for each class } m$'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\hat{\\pi}_m = {\\dfrac {n_m} n} \\ \\text{priors}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\Sigma_m = {\\dfrac 1 {n_m}} \\sum_{i: \\ y_i=m} (x_i - \\hat{\\mu}_m)(x_i - \\hat{\\mu}_m)^T$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum((x_i - mu_m) @ (x_i - mu_m).T for x_i in unique_class) / n_m)\n"
     ]
    }
   ],
   "source": [
    "display(Latex(r'$\\hat{\\pi}_m = {\\dfrac {n_m} n} \\ \\text{priors}$'))\n",
    "# display(Latex(r'$$'))\n",
    "display(Latex(r'$\\Sigma_m = {\\dfrac 1 {n_m}} \\sum_{i: \\ y_i=m} (x_i - \\hat{\\mu}_m)(x_i - \\hat{\\mu}_m)^T$'))\n",
    "# Compute covariance manually for class m\n",
    "# display(Latex(r'$$'))\n",
    "print(\"sum((x_i - mu_m) @ (x_i - mu_m).T for x_i in unique_class) / n_m)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuassianMixtureModel:\n",
    "    def __init__(self, guassian_components: int):\n",
    "        self.n_components = guassian_components\n",
    "        self.means: list = None\n",
    "        self.covariances: list = None\n",
    "        self.priors: list = None\n",
    "    \n",
    "    def plot_contours(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "class SupervisedGMM(GuassianMixtureModel):\n",
    "    \n",
    "    def fit(self, X, Y) -> None:\n",
    "        \"\"\"\n",
    "        Estimate the GMM parameters (means ð, covariances ðšº, priors ðœ‹) for each class m using labeled data.\n",
    "\n",
    "  \n",
    "        Args:\n",
    "            X :ndarray of shape (n_samples, n_features)\n",
    "                Our data which is a matrix where each row is a data point and each\n",
    "                column corresponds to features\n",
    "                \n",
    "            Y : ndarray of shape (n_samples,)\n",
    "                The class label for each data point in 'X'. Each unique value \n",
    "                in 'Y' is treated as a seperate Guassian component\n",
    "        \n",
    "        Returns\n",
    "        None\n",
    "            This method sets the attributes (They are all lists of the same length):\n",
    "            - self.means:  ndarray with the shape (n_components, n_features)\n",
    "                 Mean vector for each class.\n",
    "            - self.covariances:  ndarray with the shape (n_components, n_features, n_features)\n",
    "                One covariance matrix per class.\n",
    "            - self.priors:  ndarray with the shape (n_components,)\n",
    "                Prior probability for each class.\n",
    "        \"\"\"\n",
    "        # This is our theta Î¸\n",
    "        self.means, self.covariances, self.priors = [],[],[]\n",
    "        # Find all unique label in our Y \n",
    "        unique_labels = np.unique(Y)\n",
    "        for unique_class in unique_labels:\n",
    "            # Boolean indexing to select specific data based on their class\n",
    "            X_group = X[Y == unique_class]\n",
    "            mu_m = np.mean(X_group, axis=0)\n",
    "            self.means.append(mu_m)\n",
    "            # It's tempting to do -> self.covariances = np.cov(X_group, rowvar=False)\n",
    "            # But lets's do it according to our mathematical definition of covariance matrix for each class\n",
    "            n_m = len(X_group)\n",
    "            # We reshape to get a column vector\n",
    "            self.covariances.append(sum((x_i - mu_m).reshape(-1,1) @ (x_i - mu_m).reshape(-1,1).T for x_i in unique_class) / n_m)\n",
    "            self.priors.append(len(X_group) / len(X))\n",
    "            \n",
    "            # Convert to numpy datatype for efficiency\n",
    "            self.means = np.array(self.means)\n",
    "            self.covariances = np.array(self.covariances)\n",
    "            self.priors = np.array(self.priors)\n",
    "    \n",
    "    def predict(self, X) -> list:\n",
    "        \"\"\"\n",
    "        Predict the class labels for a set of input data points using the trained GMM\n",
    "\n",
    "        Args:\n",
    "            X :ndarray of shape (n_samples, n_features)\n",
    "                Our data which is a matrix where each row is a data point and each\n",
    "                column corresponds to features\n",
    "\n",
    "        Returns:\n",
    "            y_pred : ndarray of shape (n_samples,)\n",
    "                The predicted class label for each data point in `X`. Each label corresponds\n",
    "                to the Gaussian component with the highest posterior probability:\n",
    "        \"\"\"\n",
    "        # We create a likelihood matrix that contains the likelihood of eaxh x for every class m\n",
    "        likelihood = np.zeros((len(X), self.n_components))\n",
    "        for m in range(self.n_components):\n",
    "            # Now we invoke our unormalised posterior probability\n",
    "            likelihood[:, m] = self.priors[m] * multivariate_normal.pdf(X,self.means[m], self.covariances[m])\n",
    "        \n",
    "        # Now we retrieve the predicted class index per data point\n",
    "        return np.argmax(likelihood, axis=1)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
